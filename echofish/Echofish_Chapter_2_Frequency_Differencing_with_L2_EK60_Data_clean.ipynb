{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QszU1IfGujaI"
      },
      "source": [
        "# Frequency Differencing with L2 EK60 Data\n",
        "\n",
        "- Creators: Rudy Klucik<sup>1</sup>, Veronica Martinez<sup>1</sup>, Charles Anderson<sup>1</sup>, and Carrie Wall<sup>1</sup>\n",
        "- Affiliations: <sup>1</sup>Cooperative Institute for Research in Environmental Sciences ([CIRES](https://cires.colorado.edu/))\n",
        "- History:\n",
        "    - Version 1, 2024-09-27\n",
        "    - Version 1.1, 2024-11-01"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnztwlcsG-cy"
      },
      "source": [
        "## Overview\n",
        "The size and physiological characteristics of marine organisms determine their reflectance properties. An organisms backscatter also depends on the sonar frequency. Frequency differencing of the sonar's volume backscattering strength (Sv) called \"dB differencing\" is a common analysis technique to help differentiate sources of acoustic scatterers.\n",
        "\n",
        "This notebook demonstrates the use of frequency differencing on some Level 2 EK60 data.\n",
        "\n",
        "The Level 2 data can be found here:\n",
        "\n",
        "*   https://noaa-wcsd-zarr-pds.s3.us-east-1.amazonaws.com/level_2/Bell_M._Shimada/SH1507/EK60/SH1507.zarr/\n",
        "\n",
        "\n",
        "The files can be explored by navigating to the following AWS file explorer:\n",
        "\n",
        "*   https://noaa-wcsd-zarr-pds.s3.amazonaws.com/index.html#level_2/Bell_M._Shimada/SH1507/EK60/SH1507.zarr/\n",
        "\n",
        "## Prerequisites\n",
        "To successfully navigate and use this notebook, you should be familiar with:\n",
        "\n",
        "- the basics of Python programming such as loading modules, assigning variables, and list/array indexing\n",
        "- plotting data using matplotlib\n",
        "- EK60 water-column sonar file formats (see Chapter 1)\n",
        "\n",
        "## Learning Outcomes\n",
        "By working through this notebook, you will learn how to:\n",
        "\n",
        "- use Xarray to read cloud-native Zarr formatted files\n",
        "- apply frequency differencing to analyze the data\n",
        "\n",
        "## Time Estimate\n",
        "- Estimated text reading time: 7 to 13 min\n",
        "- Estimated code reading time: 3 to 5 min\n",
        "- Estimated total reading time: 10 to 18 min\n",
        "\n",
        "## Software\n",
        "This tutorial uses the Python programming language and packages. We will use:\n",
        "- `boto3` to access data from an S3 bucket\n",
        "- `Zarr` to read subsets of data from an S3 store\n",
        "- `Xarray` to work with Zarr data for analysis\n",
        "- `Numpy` for simple array operations\n",
        "- `Matplotlib` to plot data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8OW2VnOusyB"
      },
      "source": [
        "## Installing and importing libraries\n",
        "\n",
        "We will first need to install a couple python libraries for accessing and processing data. The following code check if the libraries are already installed in your environment and install the missing libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kb6gm8EtuhiB"
      },
      "outputs": [],
      "source": [
        "# sys provides access to system level information (i.e., the executable for the Python installation)\n",
        "import sys\n",
        "# subprocess makes system level calls\n",
        "import subprocess\n",
        "\n",
        "# List of packages used in this notebook\n",
        "PACKAGES = [\"boto3\", \"numpy\", \"matplotlib\", \"xarray\", \"zarr\", \"s3fs\"]\n",
        "\n",
        "# Loop through each package to either import or install\n",
        "for package in PACKAGES:\n",
        "    try:\n",
        "        # First, attempt to import the package\n",
        "        __import__(package)\n",
        "    except ImportError:\n",
        "        # If package import is unsuccessful, install using pip\n",
        "        # The command structure is <python executable> -m pip install <package>\n",
        "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', package])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGNbYlkjb5DC"
      },
      "source": [
        "Next import the libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XgCFLGHFb7dU"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import boto3\n",
        "import zarr\n",
        "import xarray as xr\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBzciPD0vOuY"
      },
      "source": [
        "To access the data we will use the library called \"boto3.\" This is a library that gives you access to a full suite of AWS tools, we are just going to use it to download a file from an S3 bucket. Because this is a public dataset it can be accessed using and anonymous configuration (e.g. the \"UNSIGNED\" Config below)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "An66bET2vPb0"
      },
      "outputs": [],
      "source": [
        "from botocore import UNSIGNED\n",
        "from botocore.config import Config\n",
        "\n",
        "s3_client = boto3.client('s3', config=Config(signature_version=UNSIGNED))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "86bgVzvivVIx"
      },
      "outputs": [],
      "source": [
        "WCSD_BUCKET_NAME = 'noaa-wcsd-zarr-pds'\n",
        "\n",
        "zarr_file = 'level_2/Bell_M._Shimada/SH1507/EK60/SH1507.zarr'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MgPIUHIvZkv"
      },
      "source": [
        "## Read Zarr Store data using Python's Xarray\n",
        "\n",
        "For this tutorial we will read the data directly from an S3 bucket where the data is stored in a cloud-native format called \"Zarr.\"\n",
        "\n",
        "There are different ways to open data in a Zarr format using Python. While different libraries have different strengths and weaknesses, the library we will use is called \"Xarray.\"\n",
        "\n",
        "Xarray is a powerful tool for working with scientific data to do analysis and works well with other libraries including Pandas, Numpy and tools such as Dask.\n",
        "\n",
        "One thing to note about interacting with Zarr stores is that they can represent very large datasets (sometimes hundreds of GB in size) whereas typical data workflows concern only subsets of the data. Zarr is designed to efficiently serve just the subset of data you are interested in, removing the typical requirement for users to download entire datasets before beginning their analysis.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mM49CCneMgBx"
      },
      "outputs": [],
      "source": [
        "import s3fs\n",
        "s3_file_system = s3fs.S3FileSystem(anon=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4N3qImPcfqG-"
      },
      "source": [
        "The format of the files we will process follow a format as follows:\n",
        "\n",
        "> `https://<Bucket Name>.s3.amazonaws.com/data/level_2/<Ship Name>/<Cruise Name>/<Sensor Name>/<Zarr Store>`\n",
        "\n",
        "For example:\n",
        "\n",
        "> `https://noaa-wcsd-pds.s3.amazonaws.com/data/level_2/Bell_M._Shimada/SH1507/EK60/SH1507.zarr`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b8KE4DAaMvHu"
      },
      "outputs": [],
      "source": [
        "bucket_name = 'noaa-wcsd-zarr-pds'\n",
        "\n",
        "ship_name = 'Bell_M._Shimada'\n",
        "cruise_name = 'SH1507'\n",
        "sensor_name = 'EK60'\n",
        "\n",
        "zarr_store = f'{cruise_name}.zarr'\n",
        "\n",
        "s3_zarr_store_path = f\"{bucket_name}/level_2/{ship_name}/{cruise_name}/{sensor_name}/{zarr_store}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uUCNmMzifulZ"
      },
      "outputs": [],
      "source": [
        "print(s3_zarr_store_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NxTWC6FNfyT5"
      },
      "source": [
        "Next use s3fs to directly access the files in the S3 bucket as if you were accessing files that were saved on your filesystem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mC6ZJlYeMsKR"
      },
      "outputs": [],
      "source": [
        "store = s3fs.S3Map(root=s3_zarr_store_path, s3=s3_file_system, check=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWLh4glSgAWl"
      },
      "source": [
        "Now that the \"store\" knows where the files live, we need to open the Zarr store using the desired library, Xarray."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NeGzrLiVJRe8"
      },
      "source": [
        "## Opening a Zarr store with Xarray"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymav-04SgSBN"
      },
      "source": [
        "We will open the S3 Zarr store using Xarray (the \"consolidated\" parameter just defines whether we are interested in reading the metadata in a consolidated manner, \"None\" is the desired value)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NG_VrpBmJQwA"
      },
      "outputs": [],
      "source": [
        "cruise = xr.open_zarr(store=store, consolidated=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6kISO4EngmB-"
      },
      "outputs": [],
      "source": [
        "cruise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCG81-1XOGqc"
      },
      "source": [
        "## Accessing a Level 2 Zarr Store\n",
        "\n",
        "We can see that a Level 2 Zarr store is composed of dimensions, coordinates, data variables, indices, and has some associated attributes.\n",
        "\n",
        "The main data in these level 2 Zarr stores is the acoustic measurement of volume backscattering strength denoted as \"Sv\", which has a unit of dB and is organized into a multidimensional Xarray\n",
        "DataArray. Sv by frequency along with time and depth are used to create the echograms, a visual representation of the sonar data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zs1pJXrmlb_U"
      },
      "outputs": [],
      "source": [
        "cruise.Sv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvVyWoShl1oN"
      },
      "source": [
        "The coordinates of the Sv data are defined by a depth value (meters below the sea surface), time (defined by Pandas datetime64[ns]), and frequency (data are typically measured across several frequencies meant for intercomparison)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ebv9Z0kYlv7H"
      },
      "outputs": [],
      "source": [
        "cruise.coords"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0_FzJJTmVxb"
      },
      "source": [
        "In addition to the above we also define DataArrays for spatiotemporal information associated with each Sv measurement. This includes the already defined \"time\" as well as \"latitude\" and \"longitude.\" They can each be accessed using the dot operator e.g.:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "46cm639anR-e"
      },
      "outputs": [],
      "source": [
        "cruise.time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vjiRy_H6nLqg"
      },
      "outputs": [],
      "source": [
        "cruise.latitude"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vQYxb1v_nPkb"
      },
      "outputs": [],
      "source": [
        "cruise.longitude"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6AJXWYepcEG"
      },
      "source": [
        "These coordinate conventions allow us the ability to quickly subset or sample the data in different ways so that it can be selected by frequency, time, depth, or GPS coordinates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yUPM1MegslVM"
      },
      "outputs": [],
      "source": [
        "cruise.depth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgAZ0f9uOf5f"
      },
      "source": [
        "The 'depth' defines gridded measurements of Sv values and is defined in meters below the sea surface starting at 0 meters, increasing going downward. There are typically multiple 'frequencies' such as 18 kHz, 38 kHz, 70 kHz, 120 kHz, and 200 kHz. The frequencies that are found in a file depends on the instrumentation the ship was outfitted with and the underlying science goals of the mission collecting the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xd9Pgf8gRZNO"
      },
      "source": [
        "Some of the date and depth ranges and the frequencies in the datset are:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AanLnI5vJWqb"
      },
      "outputs": [],
      "source": [
        "print(f\"Date range: '{cruise.time.values[0]}' to '{cruise.time.values[-1]}'\\n\")\n",
        "\n",
        "print(f\"Maximum depth: '{cruise.depth.values[-1]}' measured in meters.\\n\")\n",
        "\n",
        "print(f\"Frequencies: {cruise.frequency.values} measured in Hz.\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZeuuk_QfZaJ"
      },
      "source": [
        "## Data Chunking Format\n",
        "\n",
        "One thing to take note of as you begin to interact with Zarr stores is that a store is actually not a singular file but rather a directory consisting of [typically] thousands of smaller files to enable quick computation on subsets of the data.\n",
        "\n",
        "If you are interested in the underlying implementation see [this video](https://www.youtube.com/watch?v=qo0qqE7WGLQ&t=235s) for more information.\n",
        "\n",
        "The \"chunk\" size of the Sv data can be found as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gVrIelWeufT3"
      },
      "outputs": [],
      "source": [
        "cruise.Sv.chunk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEeXnw53vErJ"
      },
      "source": [
        "where the value is 512 by 512 by 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qlojZQn3Rnh-"
      },
      "source": [
        "Subsetting on one hour worth of data near an area of interest. We'll define select_times from 19:00 to 20:00 on July 19th 2015."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IQWMtsO5TInL"
      },
      "outputs": [],
      "source": [
        "# roughly covering the raw files from SaKe2015-D20150719-T193412.raw to SaKe2015-D20150719-T195842.raw\n",
        "start_time = np.datetime64('2015-07-19T19:00:00')\n",
        "end_time = np.datetime64('2015-07-19T20:00:00')\n",
        "\n",
        "select_times = (cruise.time > start_time) & (cruise.time < end_time)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qm5HGPnQTh5f"
      },
      "source": [
        "We can then use this slice of select_times to select the Sv values for the specific subset of time. Additional subsetting can also be added to the query, for example we can additionally subset by defining a selected frequency. The example below selects 18 kHz for the time interval of interest and store this in Sv_18 (drop=True means that the coordinate labels that do not meet the condition are not just masked but also dropped from the result)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SeEu8glfVOPo"
      },
      "outputs": [],
      "source": [
        "Sv_38 = cruise.sel(frequency=38000).where((select_times), drop=True).Sv\n",
        "print(Sv_38)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lyp3PaQo44Dp"
      },
      "source": [
        "## Plotting the Echograms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LVxDWxuocxCU"
      },
      "outputs": [],
      "source": [
        "Sv_38"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UWufAPARcOwW"
      },
      "outputs": [],
      "source": [
        "type(Sv_38)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EsBTJzD6cBYG"
      },
      "outputs": [],
      "source": [
        "print(f\"min: {np.nanmin(Sv_38)}, max: {np.nanmax(Sv_38)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fqnph14WyBza"
      },
      "outputs": [],
      "source": [
        "Sv_38.dropna(dim='depth').plot(yincrease=False, cmap='jet', vmin=-80, vmax=-30)\n",
        "\n",
        "plt.title(f\"Sv 38 kHz\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ka4XCWqLyWgr"
      },
      "source": [
        "The data in the plot above shows a detected bottom as a ship crosses above a smooth ridge. Beneath the seabed are two harmonics we would like to omit from the plot. We can begin to do that by just plotting data between 0 and 500 meters of depth."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NKSrQk8qy7o8"
      },
      "outputs": [],
      "source": [
        "start_depth = 0\n",
        "end_depth = 500\n",
        "\n",
        "select_depths = (cruise.depth > start_depth) & (cruise.depth < end_depth)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fkYHQXjIyrSf"
      },
      "outputs": [],
      "source": [
        "Sv_38_subset = Sv_38.where((select_depths), drop=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zJvY9ZrzwRC"
      },
      "source": [
        "Now when we plot the data it is subsetted to less than 500 meters of depth."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oTELcbk9xaPl"
      },
      "outputs": [],
      "source": [
        "Sv_38_subset.plot(yincrease=False, cmap='jet', vmin=-80, vmax=-30)\n",
        "plt.title(f\"Sv 38 kHz\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tanoiWmWnIW4"
      },
      "source": [
        "We can also select the corresponding data for 120 and 200 kHz and store them in seperate variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KZJZ7RniiGEv"
      },
      "outputs": [],
      "source": [
        "Sv_18 = cruise.sel(frequency=18000).where((select_times) & (select_depths), drop=True).Sv\n",
        "Sv_120 = cruise.sel(frequency=120000).where((select_times) & (select_depths), drop=True).Sv\n",
        "Sv_200 = cruise.sel(frequency=200000).where((select_times) & (select_depths), drop=True).Sv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpt_1zp8wqie"
      },
      "source": [
        "For comparison, let's see the 120 kHz echogram with the same depth bounds."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b7o-5PqDwivH"
      },
      "outputs": [],
      "source": [
        "Sv_120_subset = Sv_120.where((select_depths), drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EEd3BTeswePW"
      },
      "outputs": [],
      "source": [
        "Sv_120_subset.plot(yincrease=False, cmap='jet', vmin=-80, vmax=-30)\n",
        "plt.title(f\"Sv 120 kHz\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ctmDS-ljAUj"
      },
      "source": [
        "## Frequency Differencing\n",
        "\n",
        "Frequency differencing is simply subtracting the Sv values at one frequency from those at another frequency. Here we will compare the 120 kHz data selected above to the corresponding data at 38 kHz stored and plot the results. You can compare different frequencies by editing the code below and re-running this step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KgNXvnmPwuG7"
      },
      "outputs": [],
      "source": [
        "first_freq_array = Sv_120\n",
        "first_freq = int(first_freq_array.frequency.values/1000)\n",
        "\n",
        "second_freq_array = Sv_38\n",
        "second_freq = int(second_freq_array.frequency.values/1000)\n",
        "\n",
        "Sv_select_diff = first_freq_array - second_freq_array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ba2aLUzwvEc"
      },
      "outputs": [],
      "source": [
        "Sv_select_diff = Sv_select_diff.dropna(dim='depth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YujXksrnQ4LY"
      },
      "outputs": [],
      "source": [
        "Sv_select_diff.plot(yincrease=False, vmin=-15, vmax=15, cmap=\"coolwarm\") # updated -rk\n",
        "plt.title(f\"Sv difference between {first_freq} kHz - {second_freq} kHz\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5BxW6hM2W7w"
      },
      "source": [
        "Positive values (red colors) found along top of the ridge, between 100 and 200 meters of depth, and near the surface show stronger backscatter resulting from the 120 kHz transducer. This likely indicates the presence of zooplankton. Negative values (blue colors) show where scattering was greater from the 38 kHz transducer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nV3r2mdysExa"
      },
      "source": [
        "# Data statement\n",
        "All data used in this notebook are publicly available\n",
        "\n",
        "The Level 2 data can be found here:\n",
        "\n",
        "*   https://noaa-wcsd-zarr-pds.s3.us-east-1.amazonaws.com/level_2/Bell_M._Shimada/SH1507/EK60/SH1507.zarr/\n",
        "\n",
        "\n",
        "The files can be explored by navigating to the following AWS file explorer:\n",
        "\n",
        "*   https://noaa-wcsd-zarr-pds.s3.amazonaws.com/index.html#level_2/Bell_M._Shimada/SH1507/EK60/SH1507.zarr/\n",
        "\n",
        "# Acknowledgments\n",
        "Funding support was provided by the NOAA Center for Artificial Intelligence ([NCAI](https://www.noaa.gov/noaa-center-for-artificial-intelligence/)) and [NOAA Fisheries](https://www.fisheries.noaa.gov/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-Wy4liUnaUd"
      },
      "source": [
        "# References\n",
        "\n",
        "* Lawson, G.L., Wiebe, P.H., Stanton, T.K. and Ashjian, C.J., 2008. Euphausiid distribution along the Western Antarctic Peninsula—Part A: development of robust multi-frequency acoustic techniques to identify euphausiid aggregations and quantify euphausiid size, abundance, and biomass. Deep Sea Research Part II: Topical Studies in Oceanography, 55(3-4), pp.412-431. https://doi.org/10.1016/j.dsr2.2007.11.010   \n",
        "* Lee, Wu-Jung, Nguyen, Kavin, Setiawan, Landung, Mayorga, Emilio, Reyes, Brandon, Majeed, Imran, & Staneva, Valentina. (2023). echopype (v0.8.1). Zenodo. https://doi.org/10.5281/zenodo.8312077\n",
        "* Madureira, L.S., Everson, I. and Murphy, E.J., 1993. Interpretation of acoustic data at two frequencies to discriminate between Antarctic krill (Euphausia superba Dana) and other scatterers. Journal of plankton research, 15(7), pp.787-802.\n",
        "* Wall, C.C., Jech, J.M. and McLean, S.J., 2016. Increasing the accessibility of acoustic data through global access and imagery. ICES Journal of Marine Science, 73(8), pp.2093-2103."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOa9cNCcrk4D"
      },
      "source": [
        "# Metadata\n",
        "- Language / packages(s):\n",
        " - Language: Python\n",
        " - Packages: Boto, Xarray, Zarr\n",
        "\n",
        "- Scientific domain:\n",
        " - Fisheries acoustics\n",
        "\n",
        "- Application keywords:\n",
        " - Sonar processing\n",
        "\n",
        "- Geophysical keywords\n",
        " - Fish\n",
        " - Zooplankton\n",
        " - Seafloor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78ELGt0RrydD"
      },
      "source": [
        "# License\n",
        "## Software and Content Description License\n",
        "Software code created by U.S. Government employees is not subject to copyright in the United States (17 U.S.C. §105). The United States/Department of Commerce reserve all rights to seek and obtain copyright protection in countries other than the United States for Software authored in its entirety by the Department of Commerce. To this end, the Department of Commerce hereby grants to Recipient a royalty-free, nonexclusive license to use, copy, and create derivative works of the Software outside of the United States."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XhTcqMPvr3tA"
      },
      "source": [
        "# Disclaimer\n",
        "This Jupyter notebook is a scientific product and is not official communication of the National Oceanic and Atmospheric Administration, or the United States Department of Commerce. All NOAA Jupyter notebooks are provided on an 'as is' basis and the user assumes responsibility for its use. Any claims against the Department of Commerce or Department of Commerce bureaus stemming from the use of this Jupyter notebook will be governed by all applicable Federal law. Any reference to specific commercial products, processes, or services by service mark, trademark, manufacturer, or otherwise does not constitute or imply their endorsement, recommendation or favoring by the Department of Commerce. The Department of Commerce seal and logo, or the seal and logo of a DOC bureau, shall not be used in any manner to imply endorsement of any commercial product or activity by DOC or the United States Government."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}